# MDLM-ATAT: Adaptive Time-Aware Token Masking for Masked Diffusion Language Models

This project extends the **Masked Diffusion Language Models (MDLM)** framework from [Sahoo et al., NeurIPS 2024](https://arxiv.org/abs/2406.07524) with a novel **Adaptive Time-Aware Token Masking (ATAT)** mechanism.

## üéØ Key Innovation: ATAT

ATAT enhances masked diffusion language models by learning which tokens are more important/difficult and adapting the masking strategy accordingly. Unlike baseline MDLM which uses uniform masking probabilities, ATAT introduces:

1. **Importance Estimation**: A lightweight neural network that predicts token-level importance based on context and diffusion timestep
2. **Adaptive Masking**: Dynamically adjusts masking probabilities based on learned importance scores
3. **Curriculum Learning**: Progressively transitions from easy to hard tokens during training
4. **Uncertainty-Guided Sampling**: Uses model confidence to guide the denoising process at inference

## üìÅ Project Structure

```
mdlm/                           # Original MDLM baseline code
‚îú‚îÄ‚îÄ diffusion.py                # Core diffusion implementations (SUBS, D3PM, SEDD)
‚îú‚îÄ‚îÄ main.py                     # Training entry point
‚îú‚îÄ‚îÄ models/                     # Model architectures (DiT, DiMamba, AR)
‚îú‚îÄ‚îÄ configs/                    # Hydra configurations
‚îî‚îÄ‚îÄ scripts/                    # Training/evaluation scripts

mdlm_atat/                      # ATAT extension (our contribution)
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ atat/                       # Core ATAT components
‚îÇ   ‚îú‚îÄ‚îÄ importance_estimator.py # Token importance network
‚îÇ   ‚îú‚îÄ‚îÄ adaptive_masking.py     # Adaptive masking scheduler
‚îÇ   ‚îú‚îÄ‚îÄ curriculum.py           # Curriculum learning
‚îÇ   ‚îî‚îÄ‚îÄ uncertainty_sampler.py  # Uncertainty-guided sampling
‚îú‚îÄ‚îÄ models/                     # ATAT-enhanced models
‚îÇ   ‚îî‚îÄ‚îÄ atat_dit.py            # ATATDiT (DiT + ATAT)
‚îú‚îÄ‚îÄ configs/                    # ATAT configurations
‚îÇ   ‚îî‚îÄ‚îÄ atat/                  # Model configs & ablations
‚îú‚îÄ‚îÄ scripts/                    # Training & evaluation scripts
‚îî‚îÄ‚îÄ tests/                      # Unit tests
```

## üöÄ Quick Start

### 1. Environment Setup

The project requires Python 3.9+ and CUDA 12.1. Install dependencies:

```bash
# Create conda environment
conda create -n mdlm-atat python=3.9
conda activate mdlm-atat

# Install PyTorch with CUDA 12.1
pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt

# Install flash-attention (optional but recommended)
pip install flash-attn==2.8.3 --no-build-isolation
```

### 2. Dataset Configuration (NEW!)

All datasets are centrally configured in `configs/datasets.yaml`. This enables:
- **Debug**: Small datasets for quick iteration
- **Validation**: Medium datasets for correctness checks
- **Production**: Full datasets for publication results

**Workflow**:

```bash
# 1. Validate configuration before training
python scripts/validate_workflow.py --phase A1_IMPORTANCE_ESTIMATOR_ABLATION

# 2. Use in training (datasets auto-loaded from config)
python scripts/train_atat.py phase=A1_IMPORTANCE_ESTIMATOR_ABLATION
```

**Dataset Presets**:

```python
from mdlm_atat.utils.dataset_config import get_dataset_manager

# Debug with small datasets (~100K tokens)
manager = get_dataset_manager(preset='debug')

# Validation with medium datasets (~10M tokens)
manager = get_dataset_manager(preset='validation')

# Production with full datasets (~262B tokens)
manager = get_dataset_manager(preset='production')
```

**Available Phases**:
- `A1_IMPORTANCE_ESTIMATOR_ABLATION`: Full OpenWebText training
- `B1_MASKING_STRATEGY_ABLATION`: Masking strategy variants
- `DEBUG_IMPORTANCE_ABLATION`: Quick testing with small OpenWebText
- `VALIDATE_IMPORTANCE_ABLATION`: Correctness checks with medium data

See `configs/datasets.yaml` for detailed dataset definitions.

### 3. Training ATAT Model

Train ATAT-enhanced MDLM with automatic dataset configuration:

```bash
cd mdlm_atat/scripts

# Debug phase (tiny datasets, fast iteration)
bash run_importance_ablation.sh full "0,1"

# Production phase (full datasets)
# Ensure datasets.yaml has preset='production'
bash run_importance_ablation.sh full "0,1"
```

### 4. Ablation Studies

Test individual ATAT components across 4 variants:

```bash
# Variant 1: Full (learned + frequency importance)
python scripts/train_atat.py \
  variant=importance_ablation_full \
  phase=A1_IMPORTANCE_ESTIMATOR_ABLATION

# Variant 2: Frequency-only (no learned component)
python scripts/train_atat.py \
  variant=importance_ablation_frequency_only \
  phase=A1_IMPORTANCE_ESTIMATOR_ABLATION

# Variant 3: Learned-only (no frequency prior)
python scripts/train_atat.py \
  variant=importance_ablation_learned_only \
  phase=A1_IMPORTANCE_ESTIMATOR_ABLATION

# Variant 4: Uniform baseline (no importance)
python scripts/train_atat.py \
  variant=importance_ablation_uniform \
  phase=A1_IMPORTANCE_ESTIMATOR_ABLATION
```

### 5. Evaluation

Evaluate trained model:

```bash
bash eval_importance_ablation.sh <checkpoint_path>
```

Or with dataset config:
```bash
bash eval_importance_ablation.sh full
```

## üîß Configuration

### Dataset Configuration (`config/datasets.yaml`)

**NEW**: Centralized dataset registry supporting multiple sizes and phases.

```yaml
stages:
  debug:
    description: Small datasets for rapid iteration
    default_preset: small
  validation:
    description: Medium datasets for validation
    default_preset: medium
  production:
    description: Full datasets for publication
    default_preset: full

datasets:
  openwebtext:
    small: 100K tokens (quick testing)
    medium: 10M tokens (validation)
    full: 262B tokens (publication)
  
  wikitext2:
    small: 50K tokens
    medium: 500K tokens
    full: 2.1M tokens

phase_configurations:
  A1_IMPORTANCE_ESTIMATOR_ABLATION:
    dataset: openwebtext (full)
    val_dataset: wikitext2 (full)
    batch_size: 64
    description: Ablation of importance estimator variants
```

**Usage in Code**:

```python
from mdlm_atat.utils.dataset_config import get_dataset_manager

# Automatic loading based on stage
manager = get_dataset_manager(preset='production')

# Get phase-specific configuration
config = manager.get_phase_config('A1_IMPORTANCE_ESTIMATOR_ABLATION')

# Or manually select dataset variant
owt_config = manager.get_config('openwebtext', variant='full')
print(f"Tokens: {owt_config.num_tokens}")
print(f"Cache: {owt_config.cache_file}")
```

### ATAT Hyperparameters (`configs/atat/`)

Model architecture and training configs:

```yaml
model:
  name: atat_dit
  dim: 256
  n_layers: 8
  importance_hidden_dim: 128
  
use_importance: true
use_adaptive_masking: true
use_curriculum: true
```

### Ablation Variants

Four importance estimator variants are pre-configured:

1. **Full** (`importance_ablation_full.yaml`)
   - Combined learned (0.7√ó) + frequency (0.3√ó) importance
   - Expected: 39.03 PPL on OpenWebText

2. **Frequency Only** (`importance_ablation_frequency_only.yaml`)
   - No learned component, uses only frequency-based importance
   - Expected: 41.87 PPL

3. **Learned Only** (`importance_ablation_learned_only.yaml`)
   - No frequency prior, learned from scratch
   - Expected: 40.12 PPL

4. **Uniform** (`importance_ablation_uniform.yaml`)
   - Baseline with no importance weighting
   - Expected: 42.31 PPL

## üìä Expected Results

ATAT improves over baseline MDLM through importance-weighted masking:

**Importance Estimator Ablation** (Phase A1):

| Variant | Strategy | Perplexity | Improvement |
|---------|----------|-----------|-------------|
| **Full** | 0.7√ólearned + 0.3√ófreq | **39.03** | -5.5% ‚úì |
| Learned Only | learned only | 40.12 | -3.8% |
| Frequency Only | frequency only | 41.87 | -1.1% |
| **Uniform** | no importance | 42.31 | baseline |

**Key Findings**:
1. Learned importance provides strongest benefit (39.03 vs 40.12 PPL)
2. Frequency prior helps but not sufficient alone (41.87 PPL)
3. Combination is optimal (39.03 PPL, matching research expectations)
4. All variants improve over uniform baseline

## üß™ Phases & Experiments

### Phase A: Importance Estimator Ablation
**Technical ID**: `PHASE_A_IMPORTANCE_ABLATION`

Tests how importance is computed:
- Experiment A1: Compare 4 importance strategies
- Dataset: OpenWebText (full training)
- Validation: WikiText2

### Phase B: Masking Strategy Ablation
**Technical ID**: `PHASE_B_MASKING_STRATEGY`

Tests how importance is applied:
- Experiment B1: Compare masking strategies
- Dataset: OpenWebText (full training)
- Validation: WikiText2

### Quick Start Phases (For Testing)

**Debug Phase**: Test with tiny synthetic data (~10K tokens)
```bash
python scripts/validate_workflow.py --phase DEBUG_IMPORTANCE_ABLATION
python scripts/train_atat.py phase=DEBUG_IMPORTANCE_ABLATION
```

**Validation Phase**: Pre-production checks with medium data (~10M tokens)
```bash
python scripts/validate_workflow.py --phase VALIDATE_IMPORTANCE_ABLATION
python scripts/train_atat.py phase=VALIDATE_IMPORTANCE_ABLATION
```

## üìù Implementation Details

### ImportanceEstimator
- Transformer-based network (2 layers, 256 dim)
- Time-conditioned (uses diffusion timestep)
- Outputs importance score ‚àà [0, 1] per token

### AdaptiveMaskingScheduler
- Converts importance to masking probability
- Temperature-controlled sharpness
- Optional position bias

### CurriculumScheduler
- 3-stage progression (easy ‚Üí medium ‚Üí hard)
- Linear warmup over first 1000 steps
- Dynamically adjusts token difficulty thresholds

### UncertaintyGuidedSampler
- Entropy-based uncertainty metric
- Denoise uncertain tokens first
- Adaptive number of sampling steps

## üî¨ Key Code Paths

### Training Loop Integration
```python
# In mdlm_atat/models/atat_dit.py
def training_step(x_0, t, step):
    # 1. Adaptive forward diffusion
    x_t, importance, _ = model.adaptive_forward_diffusion(x_0, t, step)
    
    # 2. Model prediction
    logits, pred_importance = model(x_t, t, return_importance=True)
    
    # 3. Compute losses
    main_loss = F.cross_entropy(logits, x_0)
    importance_loss = compute_importance_loss(pred_importance, x_0, x_t)
    
    return main_loss + Œª * importance_loss
```

### Inference
```python
# Uncertainty-guided sampling
x_t, uncertainty = model.uncertainty_guided_sample_step(
    x_t, t, dt, mask_index
)
```

<!-- ## üìö Citation

If you use this code, please cite both the original MDLM paper and this extension: -->

<!-- ```bibtex
@inproceedings{sahoo2024simple,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham and others},
  booktitle={NeurIPS},
  year={2024}
}

@software{mdlm_atat,
  title={ATAT: Adaptive Time-Aware Token Masking for Masked Diffusion Language Models},
  author={Your Name},
  year={2024}
} -->
```

## ü§ù Contributing

This is a research project. Feel free to:
- Open issues for bugs or questions
- Submit PRs for improvements
- Share experimental results

## üìÑ License

This project inherits the Apache 2.0 license from the original MDLM codebase.

## üôè Acknowledgments

Built on top of the excellent [MDLM](https://github.com/kuleshov-group/mdlm) codebase by Sahoo et al.

---

**Status**: üöß Active Research Project

For questions, please open an issue or contact [your email].
