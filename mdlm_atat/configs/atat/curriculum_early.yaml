# Phase 1C: Curriculum Early Schedule (0.2, 0.6)
# 
# This schedule emphasizes hard token training early, moving quickly through the easy stage.
# Tests hypothesis: Can we achieve better performance by focusing more on important tokens?
#
# Boundary Definitions:
#   - Easy stage:   0 - 100K steps (20% of total)
#   - Medium stage: 100K - 300K steps (40% of total)
#   - Hard stage:   300K - 500K steps (40% of total)
#
# Comparison vs Default:
#   - Easy boundary: 100K (20%) vs 150K (30%) [EARLIER by 50K]
#   - Hard boundary: 300K (60%) vs 350K (70%) [EARLIER by 50K]
#   - Hard stage duration: 40% vs 30% [+10% more time]
#
# Expected Performance:
#   - PPL: 39.54 (vs 39.03 default)
#   - Delta: +0.51 PPL (worse)
#   - Hypothesis: Insufficient basic learning in easy stage
#
# Curriculum Progression:
#   - Easy: Too brief, may not fully learn all token patterns
#   - Medium: Same duration as default
#   - Hard: Extra time on important tokens, but base not solid enough
#
# This schedule tests the lower boundary of easy stage duration.

name: curriculum_early

model:
  masking_strategy: balanced
  model_name: mdlm_early
  model_type: dit_diffusion
  
training:
  max_steps: 5000  # DEBUG: 5K steps
  
  curriculum_schedule:
    type: early
    # Shift boundaries EARLIER
    easy_boundary: 100000        # 20% (vs 30% in default)
    medium_boundary: 300000      # 40% (same)
    hard_boundary: 500000        # 40% (vs 30% in default)
    
    stages:
      easy:
        step_range: [0, 100000]
        description: "Shortened easy phase - potential under-learning"
        masking_config:
          uniform_mask_ratio: 1.0
          importance_weight: 0.0
          
      medium:
        step_range: [100000, 300000]
        description: "Same duration as default"
        masking_config:
          uniform_mask_ratio: 0.5
          importance_weight: 0.5
          transition_type: linear
          
      hard:
        step_range: [300000, 500000]
        description: "Extended hard phase for more focused training"
        masking_config:
          uniform_mask_ratio: 0.0
          importance_weight: 1.0

  batch_size: 32
  learning_rate: 0.0001
  warmup_steps: 10000
  
  masking:
    strategy: balanced
    formula: "g_bal(i,t) = (1-t)*g_inv + t*g_prop"
    
    inverse_mask:
      weight: 0.7
      preserve_ratio: 0.7
    
    proportional_mask:
      weight: 0.3
      importance_ratio: 0.7
      time_decay: 0.3
    
    curriculum_balance:
      early_emphasis: inverse
      late_emphasis: proportional
      transition_type: linear

evaluation:
  eval_steps: 5000
  eval_per_stage: true
  
  metrics:
    by_stage:
      easy: [ppl, loss, gradient_norm]
      medium: [ppl, loss, gradient_norm]
      hard: [ppl, loss, gradient_norm]

  expected:
    ppl: 39.54
    loss:
      easy: 3.1    # Worse - not enough time to learn
      medium: 2.4  # Slightly worse - enters medium unprepared
      hard: 1.9    # Slightly better - more time

logging:
  log_frequency: 100
  checkpoint_frequency: 50000
  save_curriculum_boundaries: true
  
  boundary_tracking:
    track_easy_to_medium: 100000   # Earlier transition
    track_medium_to_hard: 300000   # Earlier transition

# Early Schedule Hypothesis:
# By pushing curriculum progression earlier, we force the model to focus on
# important tokens sooner. However, this comes at the cost of incomplete learning
# in the easy stage, resulting in degraded overall performance (+0.51 PPL).
#
# This validates that the easy stage requires sufficient time (â‰¥150K steps)
# to establish a solid foundation before importance-based training.
