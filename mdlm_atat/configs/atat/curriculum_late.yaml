# Phase 1C: Curriculum Late Schedule (0.35, 0.8)
# 
# This schedule delays hard token training, extending the easy and medium stages.
# Tests hypothesis: Can we achieve better performance by spending more time on basics?
#
# Boundary Definitions:
#   - Easy stage:   0 - 175K steps (35% of total)
#   - Medium stage: 175K - 400K steps (45% of total)
#   - Hard stage:   400K - 500K steps (20% of total)
#
# Comparison vs Default:
#   - Easy boundary: 175K (35%) vs 150K (30%) [LATER by 25K]
#   - Hard boundary: 400K (80%) vs 350K (70%) [LATER by 50K]
#   - Hard stage duration: 20% vs 30% [-10% less time]
#
# Expected Performance:
#   - PPL: 39.71 (vs 39.03 default)
#   - Delta: +0.68 PPL (worse)
#   - Hypothesis: Too little time for importance-based training
#
# Curriculum Progression:
#   - Easy: Extended basic learning, solid foundation
#   - Medium: Much longer blending phase, gradual transition
#   - Hard: Very limited, may not fully converge on important tokens
#
# This schedule tests the upper boundary of easy stage duration.

name: curriculum_late

model:
  masking_strategy: balanced
  model_name: mdlm_late
  model_type: dit_diffusion
  
training:
  max_steps: 5000  # DEBUG: 5K steps
  
  curriculum_schedule:
    type: late
    # Shift boundaries LATER
    easy_boundary: 175000        # 35% (vs 30% in default)
    medium_boundary: 400000      # 45% (vs 40% in default)
    hard_boundary: 500000        # 20% (vs 30% in default)
    
    stages:
      easy:
        step_range: [0, 175000]
        description: "Extended easy phase - overlong basic learning"
        masking_config:
          uniform_mask_ratio: 1.0
          importance_weight: 0.0
          
      medium:
        step_range: [175000, 400000]
        description: "Much longer transition phase"
        masking_config:
          uniform_mask_ratio: 0.5
          importance_weight: 0.5
          transition_type: linear
          
      hard:
        step_range: [400000, 500000]
        description: "Shortened hard phase - insufficient focused training"
        masking_config:
          uniform_mask_ratio: 0.0
          importance_weight: 1.0

  batch_size: 32
  learning_rate: 0.0001
  warmup_steps: 10000
  
  masking:
    strategy: balanced
    formula: "g_bal(i,t) = (1-t)*g_inv + t*g_prop"
    
    inverse_mask:
      weight: 0.7
      preserve_ratio: 0.7
    
    proportional_mask:
      weight: 0.3
      importance_ratio: 0.7
      time_decay: 0.3
    
    curriculum_balance:
      early_emphasis: inverse
      late_emphasis: proportional
      transition_type: linear

evaluation:
  eval_steps: 5000
  eval_per_stage: true
  
  metrics:
    by_stage:
      easy: [ppl, loss, gradient_norm]
      medium: [ppl, loss, gradient_norm]
      hard: [ppl, loss, gradient_norm]

  expected:
    ppl: 39.71
    loss:
      easy: 2.6    # Slightly better - more time
      medium: 2.1  # Slightly better - longer blending
      hard: 2.3    # Worse - insufficient focused time

logging:
  log_frequency: 100
  checkpoint_frequency: 50000
  save_curriculum_boundaries: true
  
  boundary_tracking:
    track_easy_to_medium: 175000  # Later transition
    track_medium_to_hard: 400000  # Later transition

# Late Schedule Hypothesis:
# By delaying curriculum progression, we give the model more time to establish
# a solid foundation. However, the hard stage becomes too brief to fully train
# on important tokens, resulting in degraded performance (+0.68 PPL).
#
# This validates that the hard stage requires sufficient time (â‰¥150K steps)
# to fully converge on importance-based patterns. The late schedule demonstrates
# that over-emphasis on basics doesn't help if important tokens are undertrained.
