# @package _global_
# Phase 1B: Masking Strategy - Importance-Proportional (Ablation Variant 1)
# Description: Always mask important tokens more aggressively
# Formula: g_prop(i,t) = 0.7*i + 0.3*(1-t)
# - Masks important tokens throughout training
# - Time-based decay helps with curriculum progression
# Expected PPL: 39.87 (+0.84 vs balanced)
# Expected Per-Stage Loss: Easy=2.1, Medium=2.3, Hard=2.1
# Hypothesis: May overtrain on easy tokens early, undertrain on hard tokens

defaults:
  - base_config
  - _self_

# Model configuration
model:
  # Use full ATAT importance estimator (same as Phase 1A)
  importance_mode: "full"  # 0.7 * learned + 0.3 * frequency_prior
  importance_learned_weight: 0.7
  importance_frequency_weight: 0.3

  # Masking strategy configuration (Phase 1B specific)
  masking_strategy: "proportional"  # Variant 1: Importance-proportional
  masking_config:
    type: "importance_proportional"
    # Proportional strategy: always mask important tokens more
    # g_prop(i,t) = 0.7*i + 0.3*(1-t)
    importance_weight: 0.7
    time_weight: 0.3
    description: "Always mask important tokens, gradually increase masking over time"
    
  curriculum:
    # Standard curriculum from project_plan.md
    enabled: true
    boundaries:
      - step: 150000   # Easy/Medium boundary
        name: "medium"
      - step: 350000   # Medium/Hard boundary
        name: "hard"
    stages:
      easy:
        steps: 150000
        description: "All tokens masked proportional to importance"
      medium:
        steps: 200000
        description: "Importance-based masking with time decay"
      hard:
        steps: 150000
        description: "High importance focus with strong time decay"

# Training configuration
trainer:
  max_steps: 5000  # DEBUG: 5K steps

# Optimizer configuration
optimizer:
  type: "adamw"
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Logging configuration
wandb:
  project: "mdlm-atat-phase1b"
  name: "phase1b-masking-proportional"
  tags: ["phase1b", "ablation", "masking", "proportional"]
  notes: "Phase 1B: Importance-proportional masking - always masks important tokens more aggressively"
  config:
    masking_strategy: "proportional"
    importance_mode: "full"
    expected_ppl: 39.87
    expected_easy_loss: 2.1
    expected_medium_loss: 2.3
    expected_hard_loss: 2.1
    hypothesis: "May overtrain on easy tokens early, undertrain on hard"

# Dataset configuration
data:
  dataset_preset: "debug"  # Will be overridden by command line
  num_workers: 4
  pin_memory: true

# Reproducibility
seed: 42
