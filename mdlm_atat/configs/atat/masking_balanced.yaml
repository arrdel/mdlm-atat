# @package _global_
# Phase 1B: Masking Strategy - Balanced (Our Proposed Method)
# Description: Balanced curriculum strategy combining early preservation with late focus
# Formula: g_bal(i,t) = (1-t) * g_inv + t * g_prop
# - Early training (t→0): Preserve important tokens (g_inv dominates)
# - Late training (t→1): Focus on important tokens (g_prop dominates)
# Expected PPL: 39.03 (baseline, best performance)
# Expected Per-Stage Loss: Easy=2.8, Medium=2.2, Hard=1.8

defaults:
  - base_config
  - _self_

# Model configuration
model:
  # Use full ATAT importance estimator (same as Phase 1A)
  importance_mode: "full"  # 0.7 * learned + 0.3 * frequency_prior
  importance_learned_weight: 0.7
  importance_frequency_weight: 0.3

  # Masking strategy configuration (Phase 1B specific)
  masking_strategy: "balanced"  # Key difference from other variants
  masking_config:
    type: "curriculum_balanced"
    # Balanced strategy: early=inverse, late=proportional
    # g_bal(i,t) = (1-t) * g_inv + t * g_prop
    early_strategy: "inverse"      # Preserve important tokens early
    late_strategy: "proportional"   # Focus on important tokens late
    transition_schedule: "linear"   # Linear interpolation from early to late
    
  curriculum:
    # Standard curriculum from project_plan.md
    enabled: true
    boundaries:
      - step: 150000   # Easy/Medium boundary
        name: "medium"
      - step: 350000   # Medium/Hard boundary
        name: "hard"
    stages:
      easy:
        steps: 150000
        description: "All tokens equally important"
        masking_strategy_weight: 0.0  # Use early_strategy
      medium:
        steps: 200000
        description: "Gradual importance integration"
        masking_strategy_weight: 0.5  # Blend both strategies
      hard:
        steps: 150000
        description: "High importance focus"
        masking_strategy_weight: 1.0  # Use late_strategy

# Training configuration
trainer:
  max_steps: 5000  # DEBUG: 5K steps

# Optimizer configuration
optimizer:
  type: "adamw"
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Logging configuration
wandb:
  project: "mdlm-atat-phase1b"
  name: "phase1b-masking-balanced"
  tags: ["phase1b", "ablation", "masking", "balanced"]
  notes: "Phase 1B: Balanced masking strategy - our proposed method with curriculum-aware importance weighting"
  config:
    masking_strategy: "balanced"
    importance_mode: "full"
    expected_ppl: 39.03
    expected_easy_loss: 2.8
    expected_medium_loss: 2.2
    expected_hard_loss: 1.8

# Dataset configuration
data:
  dataset_preset: "debug"  # Will be overridden by command line
  num_workers: 4
  pin_memory: true

# Reproducibility
seed: 42
