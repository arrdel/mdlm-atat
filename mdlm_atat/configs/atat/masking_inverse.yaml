# @package _global_
# Phase 1B: Masking Strategy - Importance-Inverse (Ablation Variant 2)
# Description: Always preserve important tokens
# Formula: g_inv(i,t) = 0.7*(1-i) + 0.3*t
# - Masks unimportant tokens throughout training
# - Time-based increase helps later curriculum stages
# Expected PPL: 40.21 (+1.18 vs balanced)
# Expected Per-Stage Loss: Easy=3.2, Medium=2.1, Hard=1.9
# Hypothesis: Important tokens may be undertrained early without proper curriculum

defaults:
  - base_config
  - _self_

# Model configuration
model:
  # Use full ATAT importance estimator (same as Phase 1A)
  importance_mode: "full"  # 0.7 * learned + 0.3 * frequency_prior
  importance_learned_weight: 0.7
  importance_frequency_weight: 0.3

  # Masking strategy configuration (Phase 1B specific)
  masking_strategy: "inverse"  # Variant 2: Importance-inverse
  masking_config:
    type: "importance_inverse"
    # Inverse strategy: always preserve important tokens
    # g_inv(i,t) = 0.7*(1-i) + 0.3*t
    importance_weight: 0.7
    time_weight: 0.3
    description: "Preserve important tokens throughout, gradually increase masking difficulty"
    
  curriculum:
    # Standard curriculum from project_plan.md
    enabled: true
    boundaries:
      - step: 150000   # Easy/Medium boundary
        name: "medium"
      - step: 350000   # Medium/Hard boundary
        name: "hard"
    stages:
      easy:
        steps: 150000
        description: "Preserve important tokens, mask unimportant"
      medium:
        steps: 200000
        description: "Importance preservation with time progression"
      hard:
        steps: 150000
        description: "Strong importance preservation with high time scaling"

# Training configuration
trainer:
  max_steps: 5000  # DEBUG: 5K steps

# Optimizer configuration
optimizer:
  type: "adamw"
  weight_decay: 0.01
  betas: [0.9, 0.999]

# Logging configuration
wandb:
  project: "mdlm-atat-phase1b"
  name: "phase1b-masking-inverse"
  tags: ["phase1b", "ablation", "masking", "inverse"]
  notes: "Phase 1B: Importance-inverse masking - always preserves important tokens"
  config:
    masking_strategy: "inverse"
    importance_mode: "full"
    expected_ppl: 40.21
    expected_easy_loss: 3.2
    expected_medium_loss: 2.1
    expected_hard_loss: 1.9
    hypothesis: "Important tokens may be undertrained early without proper curriculum balance"

# Dataset configuration
data:
  dataset_preset: "debug"  # Will be overridden by command line
  num_workers: 4
  pin_memory: true

# Reproducibility
seed: 42
