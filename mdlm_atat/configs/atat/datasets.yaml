# Global Dataset Configuration
# All training and evaluation scripts load datasets ONLY through this config
# Supports multiple dataset variants and sizes for debugging/validation/production

datasets:
  # OpenWebText - Primary dataset for all ATAT training
  openwebtext:
    # Small variant: 10K documents (debugging and validation)
    small:
      name: "openwebtext_small"
      description: "10K documents from OpenWebText for debugging"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_small"
      num_tokens: "500M"
      split: "train"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "debug, validation"
    
    # Medium variant: 100K documents (scaling validation)
    medium:
      name: "openwebtext_medium"
      description: "100K documents from OpenWebText for scaling tests"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_medium"
      num_tokens: "5B"
      split: "train"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "scaling validation, ablation studies"
    
    # Full variant: 262B tokens (production training)
    full:
      name: "openwebtext_full"
      description: "Full OpenWebText corpus for production training"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_train_bs1024_wrapped.dat"
      num_tokens: "262B"
      split: "train"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "production training, SOTA comparison"

  # PTB - Penn Treebank (evaluation only)
  ptb:
    full:
      name: "ptb"
      description: "Penn Treebank for zero-shot evaluation"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/ptb_test.dat"
      num_tokens: "42K"
      split: "test"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "zero-shot evaluation"
      
  # WikiText-103 - Wikipedia articles (evaluation only)
  wikitext103:
    full:
      name: "wikitext103"
      description: "WikiText-103 for zero-shot evaluation"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext103_test.dat"
      num_tokens: "246K"
      split: "test"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "zero-shot evaluation"

  # Lambada - Cloze prediction task
  lambada:
    full:
      name: "lambada"
      description: "Lambada for zero-shot evaluation"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/lambada_test.dat"
      num_tokens: "4.9M"
      split: "test"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "zero-shot evaluation"

  # LM1B - Google 1B Word corpus
  lm1b:
    full:
      name: "lm1b"
      description: "LM1B for zero-shot evaluation"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/lm1b_test.dat"
      num_tokens: "3M"
      split: "test"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "zero-shot evaluation"

  # AG News - News classification corpus
  agnews:
    full:
      name: "agnews"
      description: "AG News for zero-shot evaluation"
      path: "/media/scratch/adele/mdlm_fresh/data_cache/agnews_test.dat"
      num_tokens: "500K"
      split: "test"
      tokenizer: "gpt2"
      context_length: 1024
      use_case: "zero-shot evaluation"

# Default configurations for different training stages
defaults:
  debug:
    dataset: "openwebtext_small"
    description: "Quick debug run with 10K documents"
    max_steps: 100
    val_interval: 50
    
  validation:
    dataset: "openwebtext_medium"
    description: "Scaling validation with 100K documents"
    max_steps: 10000
    val_interval: 1000
    
  production:
    dataset: "openwebtext_full"
    description: "Full training with 262B tokens"
    max_steps: 500000
    val_interval: 50000

# Cache directory for all preprocessed datasets
cache_root: "/media/scratch/adele/mdlm_fresh/data_cache"

# Tokenizer settings
tokenizers:
  gpt2:
    model: "gpt2"
    vocab_size: 50257
    bos_token_id: 50256
    eos_token_id: 50256
