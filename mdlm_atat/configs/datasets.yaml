# Global Dataset Configuration
# Controls dataset selection across all training, evaluation, and validation scripts
# 
# USAGE:
#   from mdlm_atat.utils import get_dataset_manager
#   manager = get_dataset_manager(preset='debug')  # or 'validation', 'production'
#   dataset_config = manager.get_config('openwebtext')

# Stage Presets
stages:
  debug:
    description: "Small datasets for debugging - fast iteration"
    dataset_variants: ["small"]
    max_samples: 1000
    use_cache: false

  validation:
    description: "Medium datasets for validation - confirm correctness"
    dataset_variants: ["medium"]
    max_samples: 100000
    use_cache: true

  production:
    description: "Full datasets for publication results"
    dataset_variants: ["full"]
    max_samples: null
    use_cache: true

# Dataset Catalog
datasets:
  # OpenWebText Variants
  openwebtext:
    name: "OpenWebText"
    source: "huggingface:openwebtext"
    description: "40GB corpus of web text"
    
    variants:
      small:
        num_tokens: 100000
        samples: 1000
        cache_file: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_small.dat"
        description: "~100K tokens for quick testing"
      
      medium:
        num_tokens: 10000000
        samples: 100000
        cache_file: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_medium.dat"
        description: "~10M tokens for validation"
      
      full:
        num_tokens: 262000000000  # 262B tokens
        samples: null
        cache_file: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_train_bs1024_wrapped.dat"
        description: "Full OpenWebText corpus for production"
    
    splits:
      train:
        cache_files:
          small: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_small.dat"
          medium: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_medium.dat"
          full: "/media/scratch/adele/mdlm_fresh/data_cache/openwebtext_train_bs1024_wrapped.dat"
      
      validation:
        cache_files:
          small: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext2_small.dat"
          medium: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext2_medium.dat"
          full: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext2_validation_bs1024_wrapped.dat"

  # WikiText2 Variants
  wikitext2:
    name: "WikiText2"
    source: "huggingface:wikitext/wikitext-2-v1"
    description: "2.1M tokens dataset from Wikipedia"
    
    variants:
      small:
        num_tokens: 50000
        samples: 500
        cache_file: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext2_small.dat"
        description: "~50K tokens for validation"
      
      medium:
        num_tokens: 500000
        samples: 5000
        cache_file: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext2_medium.dat"
        description: "~500K tokens for validation"
      
      full:
        num_tokens: 2100000
        samples: null
        cache_file: "/media/scratch/adele/mdlm_fresh/data_cache/wikitext2_validation_bs1024_wrapped.dat"
        description: "Full WikiText2 corpus"



# Default Configuration
defaults:
  dataset: "openwebtext"
  stage: "production"
  variant: "full"

# Phase-specific Dataset Configurations
# Maps research phases to their default dataset configurations

phase_configurations:
  # Phase 1A: Importance Ablation Study
  A1_IMPORTANCE_ESTIMATOR_ABLATION:
    dataset: "openwebtext"
    train_variant: "full"      # Full OpenWebText for training
    val_dataset: "wikitext2"
    val_variant: "full"        # WikiText2 for validation
    batch_size: 64
    description: "4 importance estimator variants on full OpenWebText"
  
  # Phase 1B: Masking Strategy
  B1_MASKING_STRATEGY_ABLATION:
    dataset: "openwebtext"
    train_variant: "full"
    val_dataset: "wikitext2"
    val_variant: "full"
    batch_size: 64
    description: "4 masking strategies on full OpenWebText"
  
  # Debug Phase: Quick iteration with small datasets
  DEBUG_IMPORTANCE_ABLATION:
    dataset: "openwebtext"
    train_variant: "small"
    val_dataset: "wikitext2"
    val_variant: "small"
    batch_size: 16
    description: "Debug importance ablation with small OpenWebText and WikiText2 datasets"
  
  # Validation Phase: Confirm correctness before full training
  VALIDATE_IMPORTANCE_ABLATION:
    dataset: "openwebtext"
    train_variant: "medium"
    val_dataset: "wikitext2"
    val_variant: "medium"
    batch_size: 32
    description: "Validate importance ablation with medium datasets"

# Tokenizer Configuration
tokenizer:
  default: "gpt2"
  cache_dir: "/media/scratch/adele/mdlm_fresh/tokenizer_cache"
  force_download: false

# Data Loading Parameters
data_loading:
  num_workers: 32
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
