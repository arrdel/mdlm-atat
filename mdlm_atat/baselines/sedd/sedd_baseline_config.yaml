# SEDD Baseline Configuration
# Score Entropy Discrete Diffusion with small model (125M params)

# Tokens and graph setup
tokens: 50257  # GPT-2 vocabulary size
graph:
  type: absorb  # Absorbing state graph (mask token)

# Noise schedule
noise:
  type: loglinear  # Log-linear noise schedule
  eps: 1.0e-5  # Minimum noise level

# Model configuration - Small model (125M params, matches AR baseline)
model:
  hidden_size: 768
  n_blocks: 12
  n_heads: 12
  cond_dim: 768
  dropout: 0.0
  length: 1024
  scale_by_sigma: false  # Set to true for absorbing state (optional)

# Data configuration
data:
  train: openwebtext
  valid: wikitext103
  tokenizer_name_or_path: gpt2
  cache_dir: /media/scratch/adele/mdlm_fresh/data_cache
  wrap: true
  streaming: false

# Loader settings
loader:
  batch_size: 8  # Per GPU
  global_batch_size: 512  # Total across all GPUs
  num_workers: 8
  pin_memory: true

# Training settings
training:
  ema: 0.9999  # EMA decay
  gradient_clip: 1.0
  accum: 32  # Gradient accumulation steps

# Optimizer
optim:
  lr: 3.0e-4
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01

# Learning rate scheduler
lr_scheduler:
  warmup_steps: 10000
  schedule: cosine

# Trainer settings
trainer:
  accelerator: gpu
  devices: 2
  num_nodes: 1
  precision: 16-mixed
  max_steps: 500000
  accumulate_grad_batches: 32
  gradient_clip_val: 1.0
  log_every_n_steps: 100
  val_check_interval: 10000
  limit_val_batches: 56

# Sampling settings (for evaluation)
sampling:
  predictor: analytic  # Analytic predictor (more efficient)
  steps: 128  # Number of sampling steps
  noise_removal: true  # Final denoising step

# Checkpointing
checkpointing:
  save_dir: /media/scratch/adele/mdlm_fresh/outputs/baselines/sedd_small
  every_n_train_steps: 10000
  monitor: val/loss
  mode: min
  save_top_k: 3
  save_last: true

# WandB logging
wandb:
  project: mdlm-atat-baselines
  name: sedd_small_baseline
  mode: online
  log_model: false
  tags:
    - baseline
    - sedd
    - score_entropy
    - phase2
  notes: |
    SEDD (Score Entropy Discrete Diffusion) baseline.
    Score-based diffusion using ratio estimation instead of likelihood.
    Small model (125M params) for comparison with AR baseline.
    
    Key differences from MDLM:
    - Uses score entropy loss (ratio estimation)
    - More efficient sampling with analytic predictor
    - Competitive with autoregressive models
    
    Expected PPL: 37-40 (better than MDLM, competitive with AR)
