# @package _global_
# AR Transformer Baseline using ATAT training infrastructure
# This config allows us to train AR baseline using the same pipeline as ATAT

defaults:
  - /noise: loglinear
  - /strategy: ddp
  - /data: openwebtext
  - /callbacks: [checkpoint_every_n_steps, learning_rate_monitor]
  - /lr_scheduler: cosine_decay
  - _self_

# Basic settings
mode: train
diffusion: absorbing_state
backbone: ar_transformer  # Use AR instead of DIT
parameterization: subs
time_conditioning: False
T: 0
subs_masking: False
seed: 42

# Model configuration
model:
  name: "ar_transformer"
  type: "ar"
  vocab_size: 50257
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  ffn_dim: 3072
  max_seq_len: 1024
  dropout: 0.1
  length: 1024

# Data configuration - SCRATCH DRIVE
data:
  train: openwebtext
  valid: wikitext103
  tokenizer_name_or_path: gpt2
  cache_dir: /media/scratch/adele/mdlm_fresh/data_cache
  wrap: True
  streaming: False

# Loader settings
loader:
  batch_size: 8  # Per GPU
  eval_batch_size: 4  # Per GPU for validation
  global_batch_size: 512  # 8 * 2 GPUs * 32 accumulation
  eval_global_batch_size: 8  # 4 * 2 GPUs
  num_workers: 4
  persistent_workers: true
  pin_memory: true

# Training settings
training:
  ema: 0.9999
  antithetic_sampling: False  # Not needed for AR
  importance_sampling: False  # Not needed for AR
  sampling_eps: 1e-3
  change_of_variables: False
  importance_loss_weight: 0.0  # Not needed for AR

# Trainer settings
trainer:
  num_nodes: 1
  accumulate_grad_batches: 32  # Effective batch = 8 * 2 * 32 = 512
  max_steps: 500000
  val_check_interval: 5000
  gradient_clip_val: 1.0
  precision: "16-mixed"
  log_every_n_steps: 50

# Optimizer
optim:
  name: adamw
  lr: 3.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# LR Scheduler (will use cosine_decay from defaults)
scheduler:
  warmup_steps: 10000
  eta_min: 1.0e-5

# Logging
wandb:
  project: "mdlm-baselines"
  name: "ar-transformer-baseline"
  tags: ["baseline", "autoregressive", "phase2"]
  notes: "AR transformer baseline trained with ATAT infrastructure"
  mode: "offline"

# Checkpointing
checkpoint:
  dirpath: "/media/scratch/adele/mdlm_fresh/outputs/baselines/ar_transformer"
  filename: "ar-{step:06d}-{val_loss:.4f}"
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3
  every_n_train_steps: 10000
  save_last: true
