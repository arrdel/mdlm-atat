# MDLM-ATAT Project Restructuring (December 2024)

## Overview
This document outlines the restructured codebase for optimal organization and workflow efficiency.

## Directory Structure

```
mdlm/
â”œâ”€â”€ README.md                       # Main project README
â”œâ”€â”€ requirements.yaml               # Conda environment
â”œâ”€â”€ .gitignore                      # Git ignore patterns
â”‚
â”œâ”€â”€ docs/                          # ğŸ“š All documentation (NEW)
â”‚   â”œâ”€â”€ reports/                   # Research reports from mdlm_atat/reports
â”‚   â”‚   â”œâ”€â”€ INDEX.md              # Report index
â”‚   â”‚   â”œâ”€â”€ EXECUTIVE_SUMMARY.md
â”‚   â”‚   â”œâ”€â”€ TECHNICAL_REPORT.md
â”‚   â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md
â”‚   â”‚   â”œâ”€â”€ GETTING_STARTED.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ research_proposal.tex      # CVPR paper draft
â”‚
â”œâ”€â”€ mdlm/                          # ğŸ”µ Base MDLM implementation
â”‚   â”œâ”€â”€ main.py                    # Training entry point
â”‚   â”œâ”€â”€ diffusion.py               # Core diffusion (SUBS, D3PM, SEDD)
â”‚   â”œâ”€â”€ dataloader.py              # Dataset loading
â”‚   â”œâ”€â”€ noise_schedule.py          # Noise schedules
â”‚   â”œâ”€â”€ utils.py                   # Utilities
â”‚   â”œâ”€â”€ configs/                   # Hydra configs
â”‚   â”‚   â”œâ”€â”€ config.yaml           # Base config
â”‚   â”‚   â”œâ”€â”€ model/                # Model configs (small, medium, large)
â”‚   â”‚   â”œâ”€â”€ data/                 # Dataset configs
â”‚   â”‚   â”œâ”€â”€ noise/                # Noise schedules
â”‚   â”‚   â””â”€â”€ lr_scheduler/         # Learning rate schedules
â”‚   â”œâ”€â”€ models/                    # Model architectures
â”‚   â”‚   â”œâ”€â”€ dit.py                # Diffusion Transformer (DiT)
â”‚   â”‚   â”œâ”€â”€ dimamba.py            # DiMamba (Mamba-based)
â”‚   â”‚   â”œâ”€â”€ autoregressive.py     # AR baseline
â”‚   â”‚   â””â”€â”€ ema.py                # EMA wrapper
â”‚   â””â”€â”€ scripts/                   # Training scripts
â”‚       â”œâ”€â”€ train_owt_mdlm.sh     # OpenWebText MDLM
â”‚       â”œâ”€â”€ train_lm1b_d3pm.sh    # LM1B D3PM
â”‚       â””â”€â”€ eval_owt_*.sh         # Evaluation scripts
â”‚
â”œâ”€â”€ mdlm_atat/                     # ğŸŸ¢ ATAT Extension (our contribution)
â”‚   â”œâ”€â”€ README.md                  # ATAT-specific README
â”‚   â”œâ”€â”€ setup.py                   # Package setup
â”‚   â”œâ”€â”€ requirements.txt           # Additional dependencies
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”‚
â”‚   â”œâ”€â”€ atat/                      # Core ATAT components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ importance_estimator.py   # Token importance network
â”‚   â”‚   â”œâ”€â”€ adaptive_masking.py       # Adaptive masking scheduler
â”‚   â”‚   â”œâ”€â”€ curriculum.py             # Curriculum learning
â”‚   â”‚   â””â”€â”€ uncertainty_sampler.py    # Uncertainty-guided sampling
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # ATAT-enhanced models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ atat_dit.py           # ATATDiT (DiT + ATAT)
â”‚   â”‚
â”‚   â”œâ”€â”€ configs/                   # ATAT configurations
â”‚   â”‚   â”œâ”€â”€ atat/
â”‚   â”‚   â”‚   â”œâ”€â”€ base_config.yaml  # Base ATAT config
â”‚   â”‚   â”‚   â”œâ”€â”€ tiny.yaml         # Tiny model (25M params)
â”‚   â”‚   â”‚   â”œâ”€â”€ small.yaml        # Small model (125M params)
â”‚   â”‚   â”‚   â””â”€â”€ ablations/        # Ablation configs
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ lr_scheduler/
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                   # ATAT training & evaluation
â”‚   â”‚   â”œâ”€â”€ train_atat.py         # Main training script
â”‚   â”‚   â”œâ”€â”€ eval_atat.py          # Evaluation script
â”‚   â”‚   â”œâ”€â”€ train_pipeline.py     # Full training pipeline
â”‚   â”‚   â”œâ”€â”€ run_ablation.py       # Ablation studies
â”‚   â”‚   â”œâ”€â”€ download_datasets.py  # Dataset downloader
â”‚   â”‚   â”œâ”€â”€ generate_tiny_dataset.py  # Tiny dataset for testing
â”‚   â”‚   â”œâ”€â”€ create_sampling_gif.py    # GIF visualization
â”‚   â”‚   â””â”€â”€ run_full_pipeline.sh      # Full pipeline script
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                     # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ visualization.py      # Visualization tools
â”‚   â”‚   â””â”€â”€ gif_visualization.py  # GIF generation
â”‚   â”‚
â”‚   â””â”€â”€ tests/                     # Unit tests
â”‚       â”œâ”€â”€ conftest.py
â”‚       â”œâ”€â”€ test_atat_components.py
â”‚       â””â”€â”€ test_atat_models.py
â”‚
â”œâ”€â”€ .archive/                      # ğŸ—„ï¸ Archived/deprecated files
â”‚   â””â”€â”€ (old test scripts, etc.)
â”‚
â””â”€â”€ /media/scratch/adele/          # ğŸ’¾ Data storage (external)
    â”œâ”€â”€ mdlm_data_cache/          # Dataset cache (47GB)
    â”œâ”€â”€ mdlm_fresh/               # Fresh outputs (321GB)
    â””â”€â”€ datasets/                 # Raw datasets (50GB)
```

## Key Improvements

### 1. Clean Separation
- **mdlm/**: Base MDLM implementation (untouched, stable)
- **mdlm_atat/**: ATAT extension (our innovation, active development)
- **docs/**: All documentation in one place

### 2. Removed Clutter
- âœ… Deleted all `__pycache__/` directories
- âœ… Removed `.pyc`, `.pyo` temporary files
- âœ… Consolidated reports into `docs/reports/`
- âœ… Added comprehensive `.gitignore`

### 3. Configuration Organization
- Shared configs in `mdlm/configs/` for base functionality
- ATAT-specific configs in `mdlm_atat/configs/atat/`
- All paths point to `/media/scratch/adele/` for data

### 4. Dataset Management
- **Location**: `/media/scratch/adele/`
  - `mdlm_data_cache/`: 47GB (HuggingFace cache)
  - `mdlm_fresh/`: 321GB (outputs, checkpoints)
  - `datasets/`: 50GB (raw datasets)
- All config files updated to point to correct paths
- Training outputs saved to scratch, not cluttering repo

## Quick Reference

### Training ATAT Models
```bash
# Tiny model (fast testing)
python mdlm_atat/scripts/train_atat.py --config-name atat/tiny

# Small model (production)
python mdlm_atat/scripts/train_atat.py --config-name atat/small

# With custom config
python mdlm_atat/scripts/train_atat.py \
  --config-name atat/small \
  data.train=openwebtext \
  trainer.max_steps=100000
```

### Evaluation
```bash
# Evaluate trained model
python mdlm_atat/scripts/eval_atat.py \
  --checkpoint /media/scratch/adele/mdlm_fresh/checkpoints/model.ckpt

# Run ablation studies
python mdlm_atat/scripts/run_ablation.py
```

### Dataset Setup
```bash
# Download datasets to scratch
python mdlm_atat/scripts/download_datasets.py \
  --output-dir /media/scratch/adele/datasets \
  --datasets openwebtext wikitext103

# Generate tiny dataset for testing
python mdlm_atat/scripts/generate_tiny_dataset.py
```

## What Was Removed/Archived

### Deleted
- All `__pycache__/` directories (27 total)
- Compiled Python files (`.pyc`, `.pyo`)
- Temporary editor files (`*~`, `.DS_Store`)

### Archived to `.archive/`
- Old test scripts (test_importance_estimator.py, quick_importance_test.py)
- Redundant/experimental code

### Consolidated
- 9 report files from `mdlm_atat/reports/` â†’ `docs/reports/`
- Documentation scattered across README files â†’ organized in `docs/`

## Next Steps for Development

### Immediate (Ready to Run)
1. **Start training runs**:
   ```bash
   python mdlm_atat/scripts/train_atat.py --config-name atat/tiny --max-steps 10000
   ```

2. **Validate installation**:
   ```bash
   python -c "import mdlm_atat; print('ATAT imported successfully')"
   ```

3. **Test dataset access**:
   ```bash
   ls -lh /media/scratch/adele/mdlm_data_cache/
   ```

### Short-term (Next Week)
1. Run ATAT-Tiny training (100k steps, ~12 hours)
2. Generate baseline MDLM results for comparison
3. Run ablation studies (no-importance, no-curriculum, etc.)
4. Create visualizations for paper

### Medium-term (Next Month)
1. Full ATAT-Small training on OpenWebText
2. Complete experimental results for research proposal
3. Generate figures and update `docs/research_proposal.tex`
4. Submit paper to CVPR 2026

## Dependencies

All dependencies managed via conda:
```bash
conda env create -f requirements.yaml
conda activate mdlm
```

Additional ATAT dependencies:
```bash
pip install -e mdlm_atat/
```

## Data Paths Configuration

All configs use scratch paths:
- **Data cache**: `/media/scratch/adele/mdlm_data_cache/`
- **Outputs**: `/media/scratch/adele/mdlm_fresh/outputs/`
- **Checkpoints**: `/media/scratch/adele/mdlm_fresh/checkpoints/`

Update these in config files if your scratch location changes.

## Development Workflow

1. **Make changes** in `mdlm_atat/` (never touch `mdlm/` unless fixing bugs)
2. **Test locally** with tiny dataset first
3. **Run on scratch** for full experiments
4. **Document results** in `docs/reports/`
5. **Update paper** in `docs/research_proposal.tex`

## Important Notes

- âš ï¸ **Never commit large files**: Outputs, checkpoints, datasets stay in scratch
- âš ï¸ **Use git LFS** for any binary files in repo if needed
- âš ï¸ **Test first**: Always use tiny config before full runs
- âš ï¸ **Monitor scratch space**: 418GB used of available capacity

---

**Last Updated**: December 3, 2024  
**Maintainer**: Adele Chinda  
**Status**: Restructured and ready for training runs
