nohup: ignoring input

[0;34m============================================================[0m
[0;34m               MDLM ATAT Training with WandB                [0m
[0;34m============================================================[0m

[0;32mâœ“[0m Datasets found in: /media/scratch/adele/mdlm_fresh/data_cache

[0;36mChecking WandB authentication...[0m
[1;33mWandB not logged in. Running in offline mode...[0m

[0;36mChecking GPU availability...[0m
[0;32mâœ“[0m Found 6 GPUs
  GPU 0: NVIDIA GeForce RTX 4090 (24564 MiB total, 24108 MiB free)
  GPU 1: NVIDIA GeForce RTX 4090 (24564 MiB total, 24108 MiB free)
  GPU 2: NVIDIA GeForce RTX 4090 (24564 MiB total, 24110 MiB free)
  GPU 3: NVIDIA GeForce RTX 4090 (24564 MiB total, 24110 MiB free)
  GPU 4: NVIDIA GeForce RTX 4090 (24564 MiB total, 24110 MiB free)
  GPU 5: NVIDIA GeForce RTX 4090 (24564 MiB total, 24110 MiB free)

[0;34mTraining Configuration:[0m
  Model: [0;32matat/wikitext103_validation[0m
  Max steps: [0;32m50000[0m
  Validation interval: [0;32m5000[0m
  Batch size: [0;32m85[0m
  Learning rate: [0;32m0.0003[0m
  GPUs: [0;32m2[0m
  Log file: [0;32m/media/scratch/adele/mdlm_fresh/logs/training_20251208_061857.log[0m
  Run name: [0;32matat_owt_20251208_061857[0m

[0;34m============================================================[0m
[0;34m                    Starting Training...                    [0m
[0;34m============================================================[0m

Monitor training:
  [0;36mLogs:[0m   tail -f /media/scratch/adele/mdlm_fresh/logs/training_20251208_061857.log
  [0;36mGPUs:[0m   watch -n 1 nvidia-smi
  [0;36mWandB:[0m  https://wandb.ai

/home/adelechinda/miniconda3/envs/mdlm-atat/lib/python3.9/site-packages/lightning/fabric/__init__.py:41: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
/home/adelechinda/miniconda3/envs/mdlm-atat/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'atat/wikitext103_validation': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
Warning: causal_conv1d not available, skipping dimamba import
Seed set to 42
CONFIG
â”œâ”€â”€ lr_scheduler
â”‚   â””â”€â”€ _target_: utils.CosineDecayWarmupLRScheduler                            
â”‚       t_in_epochs: false                                                      
â”‚       t_initial: 45000.0                                                      
â”‚       warmup_prefix: true                                                     
â”‚       warmup_lr_init: 1.0e-06                                                 
â”‚       warmup_t: 5000.0                                                        
â”‚       lr_min: 1.0e-06                                                         
â”‚                                                                               
â”œâ”€â”€ mode
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ diffusion
â”‚   â””â”€â”€ absorbing_state                                                         
â”œâ”€â”€ backbone
â”‚   â””â”€â”€ dit                                                                     
â”œâ”€â”€ parameterization
â”‚   â””â”€â”€ subs                                                                    
â”œâ”€â”€ time_conditioning
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ T
â”‚   â””â”€â”€ 0                                                                       
â”œâ”€â”€ subs_masking
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ data
â”‚   â””â”€â”€ train: wikitext103-v1                                                   
â”‚       valid: wikitext103-v1                                                   
â”‚       tokenizer_name_or_path: gpt2                                            
â”‚       cache_dir: /media/scratch/adele/mdlm_fresh/data_cache                   
â”‚       wrap: true                                                              
â”‚       streaming: false                                                        
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ name: atat_dit                                                          
â”‚       type: dit                                                               
â”‚       length: 1024                                                            
â”‚       hidden_size: 768                                                        
â”‚       cond_dim: 128                                                           
â”‚       n_blocks: 12                                                            
â”‚       n_heads: 12                                                             
â”‚       scale_by_sigma: true                                                    
â”‚       dropout: 0.1                                                            
â”‚       tie_word_embeddings: false                                              
â”‚       importance_hidden_dim: 256                                              
â”‚       importance_num_layers: 2                                                
â”‚       masking_strategy: importance                                            
â”‚       masking_temperature: 1.0                                                
â”‚       position_bias: false                                                    
â”‚       use_importance: true                                                    
â”‚       use_adaptive_masking: true                                              
â”‚       use_curriculum: true                                                    
â”‚                                                                               
â”œâ”€â”€ use_importance
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ use_adaptive_masking
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ use_curriculum
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ training
â”‚   â””â”€â”€ ema: 0.9999                                                             
â”‚       antithetic_sampling: true                                               
â”‚       importance_sampling: false                                              
â”‚       sampling_eps: 0.001                                                     
â”‚       change_of_variables: false                                              
â”‚       importance_loss_weight: 0.1                                             
â”‚                                                                               
â”œâ”€â”€ curriculum
â”‚   â””â”€â”€ curriculum_type: linear                                                 
â”‚       warmup_steps: 1000                                                      
â”‚       easy_fraction: 0.3                                                      
â”‚       hard_fraction: 0.3                                                      
â”‚       dynamic_adjustment: false                                               
â”‚                                                                               
â”œâ”€â”€ loader
â”‚   â””â”€â”€ global_batch_size: 64                                                   
â”‚       eval_global_batch_size: 64                                              
â”‚       batch_size: 85                                                          
â”‚       eval_batch_size: 8                                                      
â”‚       num_workers: 32                                                         
â”‚       pin_memory: true                                                        
â”‚                                                                               
â”œâ”€â”€ sampling
â”‚   â””â”€â”€ predictor: ddpm_cache                                                   
â”‚       steps: 128                                                              
â”‚       noise_removal: true                                                     
â”‚       num_sample_batches: 2                                                   
â”‚       num_sample_log: 2                                                       
â”‚       semi_ar: false                                                          
â”‚       stride_length: 1                                                        
â”‚       num_strides: 1                                                          
â”‚                                                                               
â”œâ”€â”€ eval
â”‚   â””â”€â”€ checkpoint_path: ''                                                     
â”‚       disable_ema: false                                                      
â”‚       compute_generative_perplexity: true                                     
â”‚       perplexity_batch_size: 8                                                
â”‚       compute_perplexity_on_sanity: false                                     
â”‚       gen_ppl_eval_model_name_or_path: gpt2-large                             
â”‚       generate_samples: true                                                  
â”‚                                                                               
â”œâ”€â”€ optim
â”‚   â””â”€â”€ weight_decay: 0.01                                                      
â”‚       lr: 0.0003                                                              
â”‚       beta1: 0.9                                                              
â”‚       beta2: 0.999                                                            
â”‚       eps: 1.0e-08                                                            
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: lightning.Trainer                                             
â”‚       accelerator: cuda                                                       
â”‚       num_nodes: 1                                                            
â”‚       devices: 2                                                              
â”‚       accumulate_grad_batches: 1                                              
â”‚       gradient_clip_val: 1.0                                                  
â”‚       precision: bf16                                                         
â”‚       num_sanity_val_steps: 2                                                 
â”‚       max_steps: 50000                                                        
â”‚       log_every_n_steps: 100                                                  
â”‚       limit_train_batches: 1.0                                                
â”‚       limit_val_batches: 1.0                                                  
â”‚       val_check_interval: 5000                                                
â”‚                                                                               
â”œâ”€â”€ strategy
â”‚   â””â”€â”€ _target_: lightning.pytorch.strategies.DDPStrategy                      
â”‚       find_unused_parameters: false                                           
â”‚                                                                               
â”œâ”€â”€ noise
â”‚   â””â”€â”€ type: loglinear                                                         
â”‚       sigma_min: 0.0001                                                       
â”‚       sigma_max: 20                                                           
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ checkpoint_monitor:                                                     
â”‚         _target_: lightning.pytorch.callbacks.ModelCheckpoint                 
â”‚         monitor: val/nll                                                      
â”‚         mode: min                                                             
â”‚         save_top_k: 1                                                         
â”‚         save_last: false                                                      
â”‚         dirpath: /media/scratch/adele/mdlm_fresh/checkpoints/checkpoints      
â”‚         filename: best                                                        
â”‚         auto_insert_metric_name: false                                        
â”‚         verbose: true                                                         
â”‚                                                                               
â””â”€â”€ checkpointing
    â””â”€â”€ save_dir: /media/scratch/adele/mdlm_fresh/checkpoints                   
        resume_from_ckpt: false                                                 
        resume_ckpt_path: /media/scratch/adele/mdlm_fresh/checkpoints/checkpoint
                                                                                
/home/adelechinda/miniconda3/envs/mdlm-atat/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-12-08 06:19:01,408][__main__][INFO] - Starting Training.
Error executing job with overrides: ['trainer.max_steps=50000', 'trainer.val_check_interval=5000', 'trainer.log_every_n_steps=100', 'trainer.accelerator=cuda', 'trainer.devices=2', 'loader.batch_size=85', 'optim.lr=0.0003', 'data.cache_dir=/media/scratch/adele/mdlm_fresh/data_cache']
Traceback (most recent call last):
  File "/home/adelechinda/home/projects/mdlm/mdlm_atat/../mdlm/main.py", line 201, in main
    _train(config, logger, tokenizer)
  File "/home/adelechinda/home/projects/mdlm/mdlm_atat/../mdlm/main.py", line 170, in _train
    train_ds, valid_ds = dataloader.get_dataloaders(
  File "/home/adelechinda/home/projects/mdlm/mdlm/dataloader.py", line 528, in get_dataloaders
    assert (config.loader.global_batch_size
AssertionError

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.

[0;34m============================================================[0m
[0;34m                      Training Failed                       [0m
[0;34m============================================================[0m

[0;31mTraining failed with exit code: 1[0m

Training logs: [0;32m/media/scratch/adele/mdlm_fresh/logs/training_20251208_061857.log[0m
Output directory: [0;32m/media/scratch/adele/mdlm_fresh/outputs[0m
WandB dashboard: [0;36mhttps://wandb.ai[0m

